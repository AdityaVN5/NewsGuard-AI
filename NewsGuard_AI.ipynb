{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Search Engine Initialization"
      ],
      "metadata": {
        "id": "hhg7JEzGeJHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install requests fake-useragent beautifulsoup4\n",
        "\n",
        "import requests\n",
        "import time\n",
        "import random\n",
        "from fake_useragent import UserAgent\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Configuration for web scraping\n",
        "SEARCH_URL = \"https://duckduckgo.com/html/\"  # Using DuckDuckGo to avoid Google's CAPTCHA\n",
        "RATE_LIMIT = (5, 10)  # Random delay between requests\n",
        "RETRIES = 3\n",
        "TIMEOUT = 10\n",
        "PROXY_LIST = []  # Add proxies if available\n",
        "\n",
        "ua = UserAgent()\n",
        "\n",
        "def get_random_proxy():\n",
        "    return random.choice(PROXY_LIST) if PROXY_LIST else None\n",
        "\n",
        "def search_web(query):\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        'User-Agent': ua.random,\n",
        "        'Referer': 'https://duckduckgo.com/',\n",
        "        'Accept-Language': 'en-US,en;q=0.5'\n",
        "    })\n",
        "\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'kl': 'us-en',\n",
        "        't': 'h_'\n",
        "    }\n",
        "\n",
        "    for attempt in range(RETRIES):\n",
        "        proxy = get_random_proxy()\n",
        "        proxies = {'http': proxy, 'https': proxy} if proxy else None\n",
        "\n",
        "        try:\n",
        "            response = session.get(\n",
        "                SEARCH_URL,\n",
        "                params=params,\n",
        "                proxies=proxies,\n",
        "                timeout=TIMEOUT\n",
        "            )\n",
        "\n",
        "            if \"CAPTCHA\" in response.text:\n",
        "                print(\"CAPTCHA detected! Waiting longer...\")\n",
        "                time.sleep(random.uniform(60, 120))\n",
        "                continue\n",
        "\n",
        "            return response.text\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
        "            time.sleep(2 ** attempt)\n",
        "\n",
        "        time.sleep(random.uniform(*RATE_LIMIT))\n",
        "\n",
        "    return None\n",
        "\n",
        "def parse_duckduckgo_results(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    results = []\n",
        "    for result in soup.select('.result__body'):\n",
        "        title = result.select_one('.result__title').get_text(strip=True) if result.select_one('.result__title') else ''\n",
        "        snippet = result.select_one('.result__snippet').get_text(strip=True) if result.select_one('.result__snippet') else ''\n",
        "        link = result.select_one('.result__url').get_text(strip=True) if result.select_one('.result__url') else ''\n",
        "        results.append({\n",
        "            'title': title,\n",
        "            'snippet': snippet,\n",
        "            'link': link\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def search_trusted_sources(query):\n",
        "    trusted_query = f\"{query}\"\n",
        "    print(f\"Search Query: {trusted_query}\")\n",
        "\n",
        "    html_content = search_web(trusted_query)\n",
        "    if not html_content:\n",
        "        return []\n",
        "\n",
        "    results = parse_duckduckgo_results(html_content)\n",
        "    #num of sites queried\n",
        "    snippets = [res['snippet'] for res in results[:10]]\n",
        "    print(f\"Snippets Returned: {snippets}\")\n",
        "    return snippets\n"
      ],
      "metadata": {
        "id": "PLSjUyZ7eHjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MiniCheck Method"
      ],
      "metadata": {
        "id": "58EvMV8IeUik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"minicheck @ git+https://github.com/Liyan06/MiniCheck.git@main\"\n",
        "from minicheck.minicheck import MiniCheck\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Initialize the fact-checking model\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "scorer = MiniCheck(model_name='flan-t5-large', cache_dir='./ckpts')\n",
        "\n",
        "def assess_credibility(claim, snippets):\n",
        "    if not snippets:\n",
        "        return {\n",
        "            \"status\": \"Unverified\",\n",
        "            \"confidence\": 0.0,\n",
        "            \"details\": \"No supporting documents found\"\n",
        "        }\n",
        "\n",
        "    # Prepare documents and claims for batch processing\n",
        "    docs = snippets\n",
        "    claims = [claim] * len(docs)\n",
        "\n",
        "    try:\n",
        "        # Get model predictions\n",
        "        pred_labels, raw_probs, _, _ = scorer.score(\n",
        "            docs=docs,\n",
        "            claims=claims,\n",
        "            chunk_size=8  # Adjust based on GPU memory\n",
        "        )\n",
        "\n",
        "        # Calculate confidence score\n",
        "        supported_probs = [p for label, p in zip(pred_labels, raw_probs) if label == 1]\n",
        "        confidence = max(supported_probs) if supported_probs else 0.0\n",
        "\n",
        "        # Determine status\n",
        "        if confidence >= 0.7:\n",
        "            status = \"Supported\"\n",
        "        elif confidence <= 0.3:\n",
        "            status = \"Contradicted\"\n",
        "        else:\n",
        "            status = \"Unverified\"\n",
        "\n",
        "        return {\n",
        "            \"status\": status,\n",
        "            \"confidence\": round(confidence, 2),\n",
        "            \"details\": {\n",
        "                \"all_probabilities\": [round(p, 2) for p in raw_probs],\n",
        "                \"predictions\": pred_labels\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in assess_credibility: {e}\")\n",
        "        return {\n",
        "            \"status\": \"Error\",\n",
        "            \"confidence\": 0.0,\n",
        "            \"details\": {\n",
        "                \"error_message\": str(e),\n",
        "                \"all_probabilities\": [],\n",
        "                \"predictions\": []\n",
        "            }\n",
        "        }\n",
        "\n",
        "def verify_news(text):\n",
        "    def extract_claims(text):\n",
        "        return [claim.strip() for claim in text.split(';') if claim.strip()]\n",
        "\n",
        "    results = []\n",
        "    claims = extract_claims(text)\n",
        "\n",
        "    for claim in claims:\n",
        "        print(f\"\\nVerifying claim: {claim}\")\n",
        "        snippets = search_trusted_sources(claim)\n",
        "\n",
        "        if len(snippets) < 3:\n",
        "            print(f\"Warning: Only {len(snippets)} sources found\")\n",
        "\n",
        "        assessment = assess_credibility(claim, snippets)\n",
        "\n",
        "        formatted_result = {\n",
        "            \"original_claim\": claim,\n",
        "            \"credibility_status\": assessment[\"status\"],\n",
        "            \"confidence_score\": assessment[\"confidence\"],\n",
        "            \"details\": {\n",
        "                \"probabilities\": assessment[\"details\"][\"all_probabilities\"],\n",
        "                \"predictions\": assessment[\"details\"][\"predictions\"],\n",
        "                \"raw_snippets\": snippets\n",
        "            }\n",
        "        }\n",
        "\n",
        "        results.append(formatted_result)\n",
        "        print(f\"Result: {assessment['status']} (Confidence: {assessment['confidence']:.2f})\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "othXer-qePbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verification"
      ],
      "metadata": {
        "id": "BJuBMcpxedq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#news_text = \"Myanmar earthquake: Thousands spend nights 'outside' fearing tremors broken roads hamper relief operations.\"\n",
        "news_text = \"Modi visited memorial of RSS founder in Nagpur and was seen in casual conversation with RSS chief Mohan Bhagwat.\"\n",
        "\n",
        "results = verify_news(news_text)"
      ],
      "metadata": {
        "id": "6G9q8fcKeeun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio\n"
      ],
      "metadata": {
        "id": "Oc8GyBuQe1Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio fuzzywuzzy minicheck"
      ],
      "metadata": {
        "id": "ogQKkHaBe2vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import nltk\n",
        "import html  # Standard HTML escaping\n",
        "import requests\n",
        "import time\n",
        "import random\n",
        "import gradio as gr\n",
        "from minicheck.minicheck import MiniCheck\n",
        "from fake_useragent import UserAgent\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- NLTK Download ---\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    print(\"Downloading NLTK 'punkt' tokenizer...\")\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# --- Configuration & Initialization ---\n",
        "print(\"Testing html.escape directly after import:\", html.escape(\"<b>Bold?</b>\")) # Test escape\n",
        "\n",
        "# Model Initialization\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
        "scorer = None\n",
        "try:\n",
        "    scorer = MiniCheck(model_name='flan-t5-large', cache_dir='./ckpts')\n",
        "    print(\"MiniCheck model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to initialize MiniCheck model: {e}\")\n",
        "    print(\"Fact-checking functionality will be disabled.\")\n",
        "\n",
        "# Web Scraping Config\n",
        "SEARCH_URL = \"https://duckduckgo.com/html/\"\n",
        "RATE_LIMIT = (7, 12)\n",
        "RETRIES = 3\n",
        "TIMEOUT = 15\n",
        "PROXY_LIST = []\n",
        "\n",
        "# User Agent Initialization\n",
        "ua = None\n",
        "try:\n",
        "    ua = UserAgent()\n",
        "    print(\"Fake UserAgent initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not initialize fake_useragent. Using a default User-Agent. Error: {e}\")\n",
        "    class FallbackUserAgent:\n",
        "        def random(self):\n",
        "            return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    ua = FallbackUserAgent()\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def get_random_proxy():\n",
        "    return random.choice(PROXY_LIST) if PROXY_LIST else None\n",
        "\n",
        "# --- Web Search Functions ---\n",
        "\n",
        "def search_web(query):\n",
        "    if not ua:\n",
        "        print(\"ERROR: UserAgent not initialized. Cannot perform web search.\")\n",
        "        return None\n",
        "\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        'User-Agent': ua.random,\n",
        "        'Referer': 'https://duckduckgo.com/',\n",
        "        'Accept-Language': 'en-US,en;q=0.5'\n",
        "    })\n",
        "    params = {'q': query, 'kl': 'us-en'}\n",
        "\n",
        "    for attempt in range(RETRIES):\n",
        "        sleep_time = random.uniform(*RATE_LIMIT)\n",
        "        print(f\"Waiting for {sleep_time:.2f} seconds before attempt {attempt + 1}...\")\n",
        "        time.sleep(sleep_time)\n",
        "        proxy = get_random_proxy()\n",
        "        proxies = {'http': proxy, 'https': proxy} if proxy else None\n",
        "\n",
        "        try:\n",
        "            print(f\"Attempt {attempt + 1}: Searching for '{query}'...\")\n",
        "            response = session.get(SEARCH_URL, params=params, proxies=proxies, timeout=TIMEOUT)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            if \"CAPTCHA\" in response.text:\n",
        "                print(\"CAPTCHA detected! Waiting longer...\")\n",
        "                time.sleep(random.uniform(60, 120))\n",
        "                continue\n",
        "\n",
        "            print(f\"Attempt {attempt + 1} successful.\")\n",
        "            return response.text\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"Attempt {attempt + 1} failed: Timeout after {TIMEOUT} seconds.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
        "\n",
        "    print(f\"Failed to retrieve search results for '{query}' after {RETRIES} attempts.\")\n",
        "    return None\n",
        "\n",
        "def parse_duckduckgo_results(html_content):\n",
        "    if not html_content:\n",
        "        return []\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        results = []\n",
        "        # Updated selectors based on potential DDG HTML structures\n",
        "        for res in soup.select('.result, .web-result, div.result'):\n",
        "            title_tag = res.select_one('.result__title a, .result-title a, h2.result-title a')\n",
        "            snippet_tag = res.select_one('.result__snippet, .result-snippet, div.result-snippet')\n",
        "            link_tag = res.select_one('.result__url, .result-link, a.result-link') # Check link text source if needed\n",
        "\n",
        "            title = title_tag.get_text(strip=True) if title_tag else 'N/A'\n",
        "            snippet = snippet_tag.get_text(strip=True) if snippet_tag else 'N/A'\n",
        "            link = None\n",
        "\n",
        "            # Prefer href from title tag, fall back to link tag's text or href\n",
        "            if title_tag and title_tag.has_attr('href'):\n",
        "                link = title_tag['href']\n",
        "            elif link_tag and link_tag.has_attr('href'):\n",
        "                link = link_tag['href']\n",
        "            elif link_tag:\n",
        "                link = link_tag.get_text(strip=True)\n",
        "\n",
        "\n",
        "            # Basic filtering and cleaning\n",
        "            if link and snippet != 'N/A' and not link.startswith('http://duckduckgo.com') and not link.startswith('/l/'):\n",
        "                # TODO: Handle relative URLs if necessary (e.g., using urljoin)\n",
        "                results.append({\n",
        "                    'title': title,\n",
        "                    'snippet': snippet,\n",
        "                    'link': link.strip()\n",
        "                })\n",
        "\n",
        "        print(f\"Parsed {len(results)} results.\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during HTML parsing: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "\n",
        "def search_trusted_sources(query):\n",
        "    print(f\"\\n--- Searching web for claim: '{query}' ---\")\n",
        "    html_content = search_web(query)\n",
        "    if not html_content:\n",
        "        print(\"Failed to get HTML content from web search.\")\n",
        "        return []\n",
        "    parsed = parse_duckduckgo_results(html_content)\n",
        "    return parsed[:10] # Limit to top 10 results\n",
        "\n",
        "# --- Credibility Assessment ---\n",
        "\n",
        "def assess_credibility(claim, snippets):\n",
        "    if not scorer:\n",
        "        print(\"ERROR: MiniCheck model not loaded. Cannot assess credibility.\")\n",
        "        return {\"status\": \"Error\", \"confidence\": 0.0, \"details\": {\"error_message\": \"Model not loaded\", \"all_probabilities\": [], \"predictions\": []}}\n",
        "\n",
        "    if not snippets:\n",
        "        print(\"No snippets found to assess credibility.\")\n",
        "        return {\"status\": \"Unverified\", \"confidence\": 0.0, \"details\": {\"all_probabilities\": [], \"predictions\": []}}\n",
        "\n",
        "    print(f\"Assessing credibility for '{claim}' using {len(snippets)} snippets...\")\n",
        "    try:\n",
        "        pred_labels, raw_probs, _, _ = scorer.score(docs=snippets, claims=[claim]*len(snippets), chunk_size=8)\n",
        "        supported_probs = [p for label, p in zip(pred_labels, raw_probs) if label == 1]\n",
        "        confidence = max(supported_probs) if supported_probs else 0.0\n",
        "\n",
        "        if confidence >= 0.7:\n",
        "            status = \"Supported\"\n",
        "        elif any(label == 0 for label in pred_labels):\n",
        "            status = \"Contradicted\"\n",
        "        else:\n",
        "            status = \"Unverified\"\n",
        "\n",
        "        print(f\"Assessment complete: Status='{status}', Confidence={confidence:.2f}\")\n",
        "        return {\n",
        "            \"status\": status,\n",
        "            \"confidence\": round(confidence, 2),\n",
        "            \"details\": {\n",
        "                \"all_probabilities\": [round(p, 2) for p in raw_probs],\n",
        "                \"predictions\": pred_labels\n",
        "            }\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during assess_credibility: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {\"status\": \"Error\", \"confidence\": 0.0, \"details\": {\"error_message\": str(e), \"all_probabilities\": [], \"predictions\": []}}\n",
        "\n",
        "# --- Main Verification Workflow ---\n",
        "\n",
        "def verify_news(text):\n",
        "    if not text or not text.strip():\n",
        "        print(\"Input text is empty.\")\n",
        "        return []\n",
        "\n",
        "    claims = [claim.strip() for claim in text.split(';') if claim.strip()]\n",
        "    if not claims:\n",
        "        print(\"No valid claims found after splitting input.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"\\n=== Verifying {len(claims)} claim(s) ===\")\n",
        "    results = []\n",
        "\n",
        "    for i, claim in enumerate(claims):\n",
        "        print(f\"\\n[Claim {i+1}/{len(claims)}]\")\n",
        "        search_results = search_trusted_sources(claim)\n",
        "        snippets = [res['snippet'] for res in search_results if res.get('snippet') and res.get('snippet') != 'N/A']\n",
        "\n",
        "        if not snippets:\n",
        "            print(\"No valid snippets obtained from search results.\")\n",
        "            assessment = {\"status\": \"Unverified\", \"confidence\": 0.0, \"details\": {\"all_probabilities\": [], \"predictions\": []}}\n",
        "            # Add sources even if assessment failed, but mark probability as N/A\n",
        "            final_sources = [{**source, 'probability': 'N/A'} for source in search_results]\n",
        "        else:\n",
        "            assessment = assess_credibility(claim, snippets)\n",
        "            probabilities = assessment[\"details\"][\"all_probabilities\"]\n",
        "            # Match probabilities back to sources\n",
        "            final_sources = []\n",
        "            for idx, source in enumerate(search_results):\n",
        "                source_data = source.copy()\n",
        "                source_data[\"probability\"] = probabilities[idx] if idx < len(probabilities) else 'N/A'\n",
        "                final_sources.append(source_data)\n",
        "\n",
        "        if assessment[\"status\"] == \"Contradicted\":\n",
        "            gr.Warning(f\"🚨 Red Alert: The claim '{claim}' appears to be contradicted by available evidence.\")\n",
        "\n",
        "        results.append({\n",
        "            \"claim\": claim,\n",
        "            \"status\": assessment[\"status\"],\n",
        "            \"confidence\": assessment[\"confidence\"],\n",
        "            \"sources\": final_sources,\n",
        "        })\n",
        "\n",
        "    print(\"\\n=== Verification Complete ===\")\n",
        "    return results\n",
        "\n",
        "# --- Formatting for Gradio Output ---\n",
        "\n",
        "def format_results(results):\n",
        "    # Test html.escape within the function context too\n",
        "    print(\"Testing html.escape inside format_results:\", html.escape(\"<h1>Test</h1>\"))\n",
        "\n",
        "    if not results:\n",
        "        # Return simple message if no results (e.g., empty input)\n",
        "        return \"<p style='color: gray; text-align: center; padding: 20px;'>Please enter a claim to analyze, or check logs for errors if input was provided.</p>\"\n",
        "\n",
        "    # Standardized CSS\n",
        "    css = \"\"\"\n",
        "    <style>\n",
        "        .header {text-align: center; padding: 20px; background: #f0f2f5; border-radius: 10px; margin-bottom: 20px; border: 1px solid #dee2e6;}\n",
        "        .header h1 { margin: 0; color: #343a40; }\n",
        "        .header p { margin: 5px 0 0; color: #6c757d; }\n",
        "        .result-box {padding: 20px; margin: 20px 0; border-radius: 10px; background: white; box-shadow: 0 4px 8px rgba(0,0,0,0.05); border: 1px solid #e9ecef;}\n",
        "        .supported {border-left: 5px solid #28a745;} /* Green */\n",
        "        .contradicted {border-left: 5px solid #dc3545;} /* Red */\n",
        "        .unverified {border-left: 5px solid #ffc107;} /* Yellow */\n",
        "        .error {border-left: 5px solid #6c757d;} /* Gray */\n",
        "        .result-box h3 { margin-top: 0; color: #495057; font-size: 1.2em; margin-bottom: 10px;}\n",
        "        .verdict-section { margin-bottom: 15px; padding-bottom: 10px; border-bottom: 1px solid #f1f3f5;}\n",
        "        .verdict-section strong { font-size: 1.1em; }\n",
        "        .status-text-supported { color: #28a745; font-weight: bold; }\n",
        "        .status-text-contradicted { color: #dc3545; font-weight: bold; }\n",
        "        .status-text-unverified { color: #ffc107; font-weight: bold; }\n",
        "        .status-text-error { color: #6c757d; font-weight: bold; }\n",
        "        .confidence-bar {height: 10px; background: #e9ecef; border-radius: 5px; margin: 8px 0; overflow: hidden;}\n",
        "        .confidence-fill-supported {height: 100%; border-radius: 5px; background: #28a745;}\n",
        "        .confidence-fill-contradicted {height: 100%; border-radius: 5px; background: #dc3545;}\n",
        "        .confidence-fill-unverified {height: 100%; border-radius: 5px; background: #ffc107;}\n",
        "        .confidence-fill-error {height: 100%; border-radius: 5px; background: #6c757d;}\n",
        "        .sources-header { margin-top: 15px; margin-bottom: 10px; color: #495057; font-weight: bold; font-size: 1.1em; border-top: 1px solid #f1f3f5; padding-top: 15px;}\n",
        "        .source-card {padding: 15px; margin-bottom: 10px; background: #f8f9fa; border-radius: 8px; border: 1px solid #e9ecef;}\n",
        "        .source-card p { margin: 5px 0; font-size: 0.95em; line-height: 1.4;}\n",
        "        .source-card strong { color: #343a40; }\n",
        "        .source-footer { margin-top: 10px; display: flex; justify-content: space-between; align-items: center; font-size: 0.9em;}\n",
        "        .source-link { color: #007bff; text-decoration: none; font-weight: 500;}\n",
        "        .source-link:hover { text-decoration: underline; }\n",
        "        .source-prob { color: #6c757d; background-color: #e9ecef; padding: 2px 6px; border-radius: 4px; font-size: 0.85em;}\n",
        "    </style>\n",
        "    \"\"\"\n",
        "\n",
        "    html_output = css\n",
        "    for result in results:\n",
        "        try:\n",
        "            # Use html.escape for dynamic content\n",
        "            claim_html = html.escape(result.get('claim', 'N/A'))\n",
        "            status = result.get('status', 'Error').lower()\n",
        "            confidence = result.get('confidence', 0.0)\n",
        "            confidence_percent = confidence * 100\n",
        "            status_display = html.escape(result.get('status', 'Error')) # Escape status text\n",
        "\n",
        "            status_text_class = f\"status-text-{status}\"\n",
        "            confidence_fill_class = f\"confidence-fill-{status}\"\n",
        "\n",
        "            html_output += f\"\"\"\n",
        "            <div class=\"result-box {status}\">\n",
        "                <h3>Claim: {claim_html}</h3>\n",
        "                <div class=\"verdict-section\">\n",
        "                    <strong>Verdict:</strong> <span class=\"{status_text_class}\">{status_display}</span>\n",
        "                    <div class=\"confidence-bar\">\n",
        "                        <div class=\"{confidence_fill_class}\" style=\"width: {confidence_percent:.0f}%\"></div>\n",
        "                    </div>\n",
        "                    <span>Confidence Score: {confidence:.2f}</span>\n",
        "                </div>\n",
        "                <div class=\"sources-header\">Supporting Evidence / Sources:</div>\n",
        "            \"\"\"\n",
        "\n",
        "            sources = result.get(\"sources\", [])\n",
        "            if not sources:\n",
        "                html_output += \"<p style='color: gray; font-style: italic;'>No specific sources were found or analyzed for this claim.</p>\"\n",
        "            else:\n",
        "                for source in sources:\n",
        "                    title_html = html.escape(source.get('title', 'No Title Provided'))\n",
        "                    snippet_html = html.escape(source.get('snippet', 'No Snippet Available'))\n",
        "                    link_url = source.get('link', '#')\n",
        "                    # Escape the URL for use in the href attribute\n",
        "                    link_href_html = html.escape(link_url)\n",
        "\n",
        "                    prob = source.get('probability', 'N/A')\n",
        "                    prob_display = f\"{prob:.2f}\" if isinstance(prob, (float, int)) else html.escape(str(prob))\n",
        "\n",
        "                    html_output += f\"\"\"\n",
        "                    <div class=\"source-card\">\n",
        "                        <p><strong>{title_html}</strong></p>\n",
        "                        <p>{snippet_html}</p>\n",
        "                        <div class=\"source-footer\">\n",
        "                            <a href=\"{link_href_html}\" target=\"_blank\" class=\"source-link\" rel=\"noopener noreferrer\">\n",
        "                                🔗 Visit Source\n",
        "                            </a>\n",
        "                            <span class=\"source-prob\">AI Score: {prob_display}</span>\n",
        "                        </div>\n",
        "                    </div>\n",
        "                    \"\"\"\n",
        "            html_output += \"</div>\" # Close result-box\n",
        "\n",
        "        except Exception as e_fmt:\n",
        "            print(f\"ERROR during formatting result for claim '{result.get('claim', 'UNKNOWN')}': {e_fmt}\")\n",
        "            # Add a fallback message in the HTML output for this specific result\n",
        "            html_output += f\"<div class='result-box error'><p>Error formatting result for claim: {html.escape(result.get('claim', 'UNKNOWN'))}. Please check logs.</p></div>\"\n",
        "\n",
        "\n",
        "    return html_output\n",
        "\n",
        "\n",
        "# --- Gradio Interface Definition ---\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=gr.themes.colors.blue, secondary_hue=gr.themes.colors.sky), title=\"NewsGuard AI\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    <div class=\"header\">\n",
        "        <h1>🛡️ NewsGuard AI</h1>\n",
        "        <p>Enter news claims (separated by semicolons ';') to analyze their credibility using AI and web search.</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            input_box = gr.Textbox(\n",
        "                label=\"Enter News Claim(s)\",\n",
        "                placeholder=\"Example: The Eiffel Tower is in Berlin; Water boils at 100°C at sea level.\",\n",
        "                lines=6,\n",
        "                elem_id=\"input-box\"\n",
        "            )\n",
        "            gr.Examples(\n",
        "                examples=[\n",
        "                    [\"COVID-19 vaccines contain microchips; The Great Wall of China is visible from the Moon\"],\n",
        "                    [\"Drinking lemon water cures cancer; Climate change is primarily caused by human activity\"],\n",
        "                    [\"The Earth is flat\"],\n",
        "                    [\"The 2024 Olympics were held in Paris\"],\n",
        "                    [\"Large Language Models can replace all human jobs next year\"]\n",
        "                ],\n",
        "                inputs=input_box,\n",
        "                label=\"Example Claims (click to use)\"\n",
        "            )\n",
        "            submit_btn = gr.Button(\"🔍 Analyze Credibility\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column(scale=3):\n",
        "            output_panel = gr.HTML(label=\"Analysis Results\", elem_id=\"results-panel\")\n",
        "\n",
        "    # Define click action chain\n",
        "    submit_btn.click(\n",
        "        fn=verify_news,\n",
        "        inputs=input_box,\n",
        "        outputs=output_panel, # Temporarily output raw results here\n",
        "        api_name=\"verify\"\n",
        "    ).then(\n",
        "        fn=format_results,     # Format the raw results from the previous step\n",
        "        inputs=output_panel,   # Input is the output of verify_news\n",
        "        outputs=output_panel   # Output the formatted HTML back to the same panel\n",
        "    )\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Gradio App Server...\")\n",
        "    # share=True creates a public link. Set to False for local-only access.\n",
        "    # server_name=\"0.0.0.0\" allows access from other devices on the same network.\n",
        "    demo.launch(share=True)\n",
        "    print(\"Gradio App Server Stopped.\")"
      ],
      "metadata": {
        "id": "M6QUgRfOf8I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gradio app.py"
      ],
      "metadata": {
        "id": "leLRsTHV-gPR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}